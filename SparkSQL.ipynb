{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM9xnkpMa27oaY2TA5GUMrn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"4TyqZoxmtNgQ","executionInfo":{"status":"ok","timestamp":1731135796001,"user_tz":-240,"elapsed":946,"user":{"displayName":"Hassan","userId":"16276191939641172894"}}},"outputs":[],"source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import avg"]},{"cell_type":"markdown","source":["Spark Session: We initialize the Spark session, which is required to work with DataFrames and Spark SQL."],"metadata":{"id":"u65AXVuYveY3"}},{"cell_type":"code","source":["# Step 1: Initialize Spark Session\n","spark = SparkSession.builder.appName(\"DataFrame_and_SQL_Example\").getOrCreate()"],"metadata":{"id":"3tyeYEm2tO94","executionInfo":{"status":"ok","timestamp":1731135823305,"user_tz":-240,"elapsed":11486,"user":{"displayName":"Hassan","userId":"16276191939641172894"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["\tLoad Data: Create a Spark DataFrame from a list of tuples with defined column names."],"metadata":{"id":"eavQkcT8vg7t"}},{"cell_type":"code","source":["# Step 2: Load Data into a DataFrame\n","# We'll use a sample dataset of employee information for demonstration\n","data = [\n","    (\"James\", \"Sales\", 3000),\n","    (\"Michael\", \"Sales\", 4600),\n","    (\"Robert\", \"Marketing\", 4100),\n","    (\"Maria\", \"Finance\", 3000),\n","    (\"James\", \"Finance\", 3300),\n","]\n","columns = [\"Employee_Name\", \"Department\", \"Salary\"]\n","df = spark.createDataFrame(data, schema=columns)"],"metadata":{"id":"WbZKkFm0uq_U","executionInfo":{"status":"ok","timestamp":1731135830342,"user_tz":-240,"elapsed":4974,"user":{"displayName":"Hassan","userId":"16276191939641172894"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["DataFrame Operations: We group data by “Department” and calculate the average salary using Spark’s built-in functions."],"metadata":{"id":"doKqhWtQvm7_"}},{"cell_type":"code","source":["# Step 3: Perform DataFrame Operations\n","# Calculate the average salary by department\n","avg_salary_df = df.groupBy(\"Department\").agg(avg(\"Salary\").alias(\"Average_Salary\"))\n","avg_salary_df.show()"],"metadata":{"id":"eqEllL1QuwfD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Register as Temporary Table: Using createOrReplaceTempView, we register the DataFrame as a temporary SQL table."],"metadata":{"id":"6PK-XN-bvqy7"}},{"cell_type":"code","source":["# Step 4: Register DataFrame as Temporary Table\n","# This allows us to query it with SQL\n","df.createOrReplaceTempView(\"employees\")"],"metadata":{"id":"hiWdaLOHuz7-","executionInfo":{"status":"ok","timestamp":1731135861981,"user_tz":-240,"elapsed":378,"user":{"displayName":"Hassan","userId":"16276191939641172894"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["Run SQL Query: Run an SQL query on the registered table to select employees in the “Sales” department with a salary above 3500."],"metadata":{"id":"ehSBgGEivu4s"}},{"cell_type":"code","source":["# Step 5: Run SQL Queries\n","# SQL query to find employees in the Sales department with a salary greater than 3500\n","sales_employees_df = spark.sql(\"SELECT Employee_Name, Salary FROM employees WHERE Department = 'Sales' AND Salary > 3500\")\n","sales_employees_df.show()"],"metadata":{"id":"zUldqxzuu2q9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Stop the Spark session\n","spark.stop()"],"metadata":{"id":"U6iuFk2Pu5yv","executionInfo":{"status":"ok","timestamp":1731135876151,"user_tz":-240,"elapsed":340,"user":{"displayName":"Hassan","userId":"16276191939641172894"}}},"execution_count":7,"outputs":[]}]}